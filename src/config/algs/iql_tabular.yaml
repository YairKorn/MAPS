# --- "Multi-Agent Pseudo-Sequential" parameters ---
name: "iql_tabular"

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0            # Initial epsilon for exploration
epsilon_finish: 0.05          # Final epsilon for exploration

# Specify learner, controller and agent
agent: "tabular"              # A RNN agent that returns its hidden state instead of its value
agent_output_type: "q"        # The output format is Q-values
double_q: False               # DDQN
learner: "tabular_learner"       # The learner for MAPS
mac: "tabular_iql"                   # The multi-agent controller for MAPS
mixer:                         # No mixing network for MAPS
target_update_interval:        # Update the target network every {} episodes
alpha: 0.3                    # Learning rate for tabular

# MAPS-specific configuration
random_ordering: False        # Re-order sequence of agents action-selection every step
decomposed_reward: True       # If True, use agent-specific reward from action model. Otherwise, use divide the joint reward between agents

# Stoachstic Environment
apply_MCTS: True              # if True, MCTS buffer is activated. Otherwise, a mean-state is calculated (deterministic -> automatically False)
MCTS_buffer_size: 8           # Size of MCTS buffer for stochastic environment
MCTS_sampling: 0              # Sample from MCTS buffer when selecting an action. 0 means "the whole filled buffer"

internal_buffer_cuda: False    # Use CUDA for internal buffer to enhance performance