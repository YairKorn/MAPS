# --- QMIX specific parameters ---

# use epsilon greedy action selector
action_selector: "multinomial"
epsilon_start: 0.5
epsilon_finish: 0.01
epsilon_anneal_time: 150000
mask_before_softmax: True

buffer_size: 8
batch_size: 8

runner: "episode" # consider parallel?

lr: 0.0005
critic_lr: 0.0005
actor_train_reps: 1
critic_td: True
adv_norm: True

epochs: 5
clip_coef: 0.2
ent_coefficient: 0.1
# update the target network every {} episodes
target_update_interval: 100

# use the Q_Learner to train
agent_output_type: "pi_logits"
learner: "ppo_learner"
optimizer: "RMSProp" #"Adam"
double_q: True
mixer: # Mixer becomes None

name: "ppo"

l = (real loss) - coefficient * (kl divergence)