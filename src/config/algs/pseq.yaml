# --- "Pseudo-Sequential" parameters ---
name: "pseq"

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0            # Initial epsilon for exploration
epsilon_finish: 0.05          # Final epsilon for exploration
epsilon_anneal_time: 150000    # Number of time steps until exploration has finished

# specify learner, controller and agent
agent: "rnn"                  # A RNN agent that returns its hidden state instead of its value
agent_output_type: "q"        # The output format is Q-values
double_q: True                # DDQN
learner: "pseq_learner"       # The learner for PSeq
mac: "pseq"                   # The multi-agent controller for PSeq
mixer:                        # No mixing network for PSeq
target_update_interval: 200   # Update the target network every {} episodes
TDn_bound:                    # TD-lambda bound on n. null -> n_agents+1
TDn_weight: 0.3               # TD-lambda weights, calculate (1-lambda), lambda(1-lambda) etc.

# PSeq-specific configuration
random_ordering: False        # Re-order sequence of agents action-selection every step
decomposed_reward: True       # When True, PSeq use reward from action model, otherwise divide the reward uniformly between all the agents

# Stoachstic Environment
apply_MCTS: False             # if True, MCTS buffer is activated. Otherwise, a mean-state is calculated (deterministic -> automatically False)
MCTS_buffer_size: 4           # Size of MCTS buffer for stochastic environment
MCTS_sampling: 0              # Sample from MCTS buffer when selecting an action. 0 means "the whole filled buffer"
