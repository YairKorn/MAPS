# Coverage environment for experiments of MAPS, including:
#   1. Standard coverage (n agents in grid world)
#   2. Adversarial coverage (with threat in the world) - stochastic world

env: "mrac"            # a grid-world for adversarial coverage

env_args:
    random_seed: null       # defines np random seed for reproducability of tests

    map: "Threat_5x5_Hard"     # A specified setup of the environment
    # RECOMMENDED MAPS: Threat_9x9_Hard || Threat_8x8_Hard || Threat_7x7_Hard || Threat_6x6_Hard || Threat_5x5_Hard || Threat_3x3_Basic
 
    random_config: False    # if enabled (True), obtacles and threats are located randomly (override obstacles\threats_location), based on:
    shuffle_config: False   # if enabled, the configuration of the area is changed every episode 
    obstacle_rate: 0.2      # ... percentage of the area contains obstacles
    threats_rate: 0.0       # ... percentage of the area contains threats using normal distribution with
    risk_avg: 0.2           # ... ... average of threats risk
    risk_std: 0.2           # ... ... standard deviation of threats risk

    observation_range: -1   # radius of observation, -1 means full observability
    observe_ids: True       # observation includes agents' ids
    observe_state: False    # whether an observation is only partial (False) or central including agent position (True)
    watch_covered: True     # observe which cells was covered and which wasn't
    watch_surface: True     # observe where obstacles and threat are located

    allow_collisions: False # "True" allow two agents to be in the same cell
    allow_stay: True        # does agents can do "nothing action"
    default_action: 4       # The "do nothing" action

    simulated_mode: False   # if enabled, agents doesn't disabled by threats

    reward_cell: 1.0        # positive reward for every cell covered (for the first time)
    reward_collision: 0.0   # negative reward for collisions
    reward_invalid: 0.0     # negative reward for trying to make an invalid move
    reward_succes: 0.0      # positive reward given if grid is fully covered
    reward_threat: 1.0      # factor for the threat reward, based on the MRAC reward function
    reward_time: -1.0       # negative reward for every timestep

    # Logging options
    log_env: True           # collect and print log of agents performance
    visualize: True         # plot a gif of progress of learning
    frame_rate: 0.2         # frame-speed of output sequence

    episode_limit: 100      # maximum number of time steps per episode

t_max: 1500000              # number of time steps of the experiment

