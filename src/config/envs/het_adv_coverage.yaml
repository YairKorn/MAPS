# Coverage environment for experiments of MAPS, including:
#   1. Standard coverage (n agents in grid world) - deterministic world
#   2. Adversarial coverage (with threat in grid world) - stochastic world

env: "het_mrac"            # a grid-world for adversarial coverage

env_args:
    random_seed: null       # defines np random seed for reproducability of tests

    map: "HScattered_Threats1"      # A specified setup of the environment
    # RECOMMENDED MAPS: HScattered_Threats2 | HScattered_Threats1
 
    random_config: False    # if enabled (True), obtacles and threats are located randomly (override obstacles\threats_location), based on:
    shuffle_config: False   # if enabled, the configuration of the area is changed every episode 
    obstacle_rate: 0.2      # ... percentage of the area contains obstacles
    threats_rate: 0.0       # ... percentage of the area contains threats using normal distribution with:
    risk_avg: 0.2           # ... ... average of threats risk
    risk_std: 0.2           # ... ... standard deviation of threats risk

    observation_range: -1   # radius of observation, -1 means full observability
    observe_ids: True       # observation includes agents' ids
    observe_state: False    # whether an observation is only partial (False) or central including agent position (True)
    watch_covered: True     # observe which cells was covered and which wasn't
    watch_surface: True     # observe where obstacles and threat are located

    allow_collisions: False # "True" allow two agents to be in the same cell
    allow_stay: True        # does agents can do "nothing action"
    default_action: 4       # The "do nothing" action

    # Enhance performances by controlling the actual risk
    reduced_punish: 1.0     # factor of the punishment of getting into obstacles; 1.0 = no reduce, 0.0 = fully reduced
    reduced_decay: 0.0      # decay of the reduced punish as part of total time; 0.0 = no decay

    reward_cell: 1.0        # positive reward for every cell covered (for the first time)
    reward_collision: 0.0   # negative reward for collisions
    reward_invalid: 0.0     # negative reward for trying to make an invalid move
    reward_succes: 25.0      # positive reward given if grid is fully covered
    reward_threat: 1.0      # factor for the threat reward, based on the MRAC reward function
    reward_time: -1.0       # negative reward for every timestep #! CHECK BEHAVIOR FOR R=1.0!!!

    # Logging options
    log_env: True           # collect and print log of agents performance
    visualize: True         # plot a gif of progress of learning
    frame_rate: 0.2         # frame-speed of output sequence

    episode_limit: 100      # maximum number of time steps per episode

t_max: 1000              # number of time steps of the experiment

