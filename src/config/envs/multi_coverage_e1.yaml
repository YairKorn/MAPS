# Coverage environment for experiments of PSeq, including:
#   1. Standard coverage (n agents in grid world)
#   2. Adversarial coverage (with threat in the world) - stochastic world

env: "mrac_e1"

env_args:
    random_seed: 0          # defines np random seed for reproducability of tests

    obstacles_location: [
        [0, 3], [1, 3], [0, 6], [1, 6], [8, 3], [9, 3], [8, 6], [9, 6],
        [3, 0], [3, 1], [3, 8], [3, 9], [6, 0], [6, 1], [6, 8], [6, 9],
        [3, 4], [4, 4], [4, 5], [4, 6], [5, 3], [5, 4], [5, 5], [6, 5]
    ]  # specified location of obstacles in the grid
    simulated_mode: False   # if enabled, agents doesn't disabled by threats
    threat_location: []     # specified location and severty of threats in the grid
    toroidal: False         # whether the world is bounded (False) or toroidal (True)
    world_shape: [10, 10]     # the shape of the grid-world [height, width]

    # This part is disabled
    random_config: False    # if enabled (True), obtacles and threats are located randomly (override obstacles\threats_location), based on:
    shuffle_config: False   # if enabled, the configuration of the area is changed every episode 
    obstacle_rate: 0.0      # ... percentage of the area contains obstacles
    threats_rate: 0.0       # ... percentage of the area contains threats using normal distribution with
    risk_avg: 0.0           # ... ... average of threats risk
    risk_std: 0.0           # ... ... standard deviation of threats risk

    observation_range: -1   # radius of observation, -1 means full observability
    observe_ids: True       # observation includes agents' ids
    observe_state: True     # whether an observation is only partial (False) or central including agent position (True)
    watch_covered: True     # observe which cells was covered and which wasn't
    watch_surface: True     # observe where obstacles and threat are located

    allow_collisions: False # "True" allow two agents to be in the same cell
    allow_stay: True        # does agents can do "nothing action"
    n_agents: 5

    agents_placement: []    # if random_placement is False, location can be specified
    random_placement: True  # location of agents is randomized every episode

    reward_cell: 1.0        # positive reward for every cell covered (for the first time)
    reward_collision: 0.0   # negative reward for collisions
    reward_invalid: 0.0     # negative reward for trying to make an invalid move
    reward_succes: 0.0      # positive reward given if grid is fully covered
    reward_threat: 1.0      # factor for the threat reward, based on the MRAC reward function
    reward_time: -1.0       # negative reward for every timestep


    episode_limit: 200      # maximum number of time steps per episode

t_max: 3000000              # number of time steps of the experiment

